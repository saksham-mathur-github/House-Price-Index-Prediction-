# -*- coding: utf-8 -*-
"""HOME LLC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gJ4TrW2bbi93M06N-0ZM5n1nsAzdLyjY
"""

# Import necessary libraries
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the supply and demand data
supply_data = pd.read_csv('supply_data.csv')
demand_data = pd.read_csv('demand_data.csv')

# Data cleaning and preprocessing for supply_data
supply_data = supply_data.dropna()
# If needed, further preprocess supply_data here

# Data cleaning and preprocessing for demand_data
demand_data = demand_data.dropna()
# Rename 'DATE' column in demand_data to match supply_data
demand_data.rename(columns={'DATE': 'Period'}, inplace=True)
# If needed, further preprocess demand_data here
# Convert 'Period' column to datetime
demand_data['Period'] = pd.to_datetime(demand_data['Period'], format='%d-%m-%Y')

# Merge supply and demand data on 'Period'
merged_data = pd.merge(supply_data, demand_data, on='Period', how='inner')

# Define the independent variables (features)
X = merged_data.drop(['Period', 'HPI'], axis=1)

# Define the target variable
y = merged_data['HPI']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Calculate correlations
correlation = merged_data.corr()['HPI']


# Initialize the Decision Tree Regressor model and fit it to the training data
model = DecisionTreeRegressor(random_state=42)
model.fit(X_train, y_train)  # Fit the model to the training data


# Create a function to update and visualize predictions
def update_predictions(**kwargs):
    # Get feature values from sliders
    feature_values = {feature: slider for feature, slider in kwargs.items()}

    # Calculate the predicted HPI
    input_data = np.array([feature_values[feature] for feature in X.columns]).reshape(1, -1)
    predicted_hpi = model.predict(input_data)[0]

    # Determine the house price category based on HPI thresholds
    house_price_category = None
    for category, threshold in thresholds.items():
        if predicted_hpi >= threshold:
            house_price_category = category
            break

    # Display the predicted HPI and house price category
    st.text(f"Predicted HPI: {predicted_hpi:.2f}")
    st.text(f"House Price Category: {house_price_category}")

    # Display images based on house price category
    if house_price_category in image_urls:
        st.markdown(f"### {house_price_category} House")
        st.image(image_urls[house_price_category], width=300)



# Define the HPI thresholds and corresponding house price categories
thresholds = {
    'House price is Expensive': 140,
    'House price is Moderate': 100,
    'House price is Reasonable': -np.inf  # Anything below 60 is reasonable
}

# Define URLs for the house images
image_urls = {
    'House price is Expensive': 'https://i.ytimg.com/vi/6GCjTeaYnjc/maxresdefault.jpg',
    'House price is Moderate': 'https://i.pinimg.com/736x/18/ff/d7/18ffd77b9c8cb7839112a115da29c00a.jpg',
    'House price is Reasonable': 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Memphis_Tennessee-2014.jpg/640px-Memphis_Tennessee-2014.jpg'
}

# Create sliders for user input
sliders = {}  # Initialize an empty dictionary

for feature in X.columns:
    min_val = float(X_test[feature].min())  # Cast min value to float
    max_val = float(X_test[feature].max())  # Cast max value to float

    sliders[feature] = st.slider(
        label=f'{feature}:',
        min_value=min_val,
        max_value=max_val,
        value=X_test[feature].mean(),
        step=0.01,
    )

# Define the models and their performance (replace with actual data)
models = {
    'Model 1': {
        'R-squared': 0.98,
        'RMSE': 4.61,
    },
    'Model 2': {
        'R-squared': 0.98,
        'RMSE': 4.65,
    },
    # Add more models and their performance here
}

# Section 4: Feature Importances
st.header("Feature Importances")

# Assuming you have computed feature importances and stored them in a DataFrame
# Modify this part to use your actual feature importances
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': np.random.rand(len(X.columns))  # Replace with actual importances
})

# Create a bar chart of feature importances
fig, ax = plt.subplots(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis', ax=ax)
st.pyplot(fig)

# Section 5: Predictions and House Price Classification
st.header("Predictions and House Price Classification")

# Create an interactive widget
update_predictions(**sliders)

# Display Streamlit app
st.text("Predicted HPI and House Price Category:")
st.text("House Price Images:")

# Initial visualization
update_predictions(**sliders)
